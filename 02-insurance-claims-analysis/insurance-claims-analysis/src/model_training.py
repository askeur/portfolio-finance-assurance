import pandas as pd
import numpy as np
import os
import joblib
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.cluster import KMeans, DBSCAN
from sklearn.metrics import (
    classification_report, confusion_matrix, roc_auc_score, roc_curve,
    accuracy_score, precision_score, recall_score, f1_score,
    silhouette_score, adjusted_rand_score
)
from sklearn.model_selection import GridSearchCV, cross_val_score
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

class InsuranceModelTrainer:
    def __init__(self):
        self.models = {}
        self.best_model = None
        self.model_results = {}
        
    def load_processed_data(self, data_path="data/processed/processed_data.csv"):
        """Charger les donn√©es preprocess√©es"""
        print("üìä Chargement des donn√©es preprocess√©es...")
        
        try:
            self.df = pd.read_csv(data_path)
            print(f"‚úÖ Donn√©es charg√©es: {self.df.shape}")
            return self.df
        except Exception as e:
            print(f"‚ùå Erreur lors du chargement: {e}")
            return None
    
    def setup_classification_models(self):
        """Initialiser les mod√®les de classification"""
        print("\nü§ñ Configuration des mod√®les de classification...")
        
        self.classification_models = {
            'RandomForest': RandomForestClassifier(random_state=42, n_jobs=-1),
            'GradientBoosting': GradientBoostingClassifier(random_state=42),
            'LogisticRegression': LogisticRegression(random_state=42, max_iter=1000),
            'SVM': SVC(random_state=42, probability=True)
        }
        
        # Grilles de param√®tres pour optimisation
        self.param_grids = {
            'RandomForest': {
                'n_estimators': [100, 200],
                'max_depth': [10, 20, None],
                'min_samples_split': [2, 5],
                'min_samples_leaf': [1, 2]
            },
            'GradientBoosting': {
                'n_estimators': [100, 200],
                'learning_rate': [0.05, 0.1, 0.2],
                'max_depth': [3, 5, 7]
            },
            'LogisticRegression': {
                'C': [0.1, 1, 10],
                'penalty': ['l1', 'l2'],
                'solver': ['liblinear', 'saga']
            },
            'SVM': {
                'C': [0.1, 1, 10],
                'kernel': ['rbf', 'linear'],
                'gamma': ['scale', 'auto']
            }
        }
        
        print(f"‚úÖ {len(self.classification_models)} mod√®les configur√©s")
    
    def setup_clustering_models(self):
        """Initialiser les mod√®les de clustering"""
        print("\nüéØ Configuration des mod√®les de clustering...")
        
        self.clustering_models = {
            'KMeans': KMeans(random_state=42, n_init=10),
            'DBSCAN': DBSCAN()
        }
        
        self.clustering_params = {
            'KMeans': {
                'n_clusters': [2, 3, 4, 5, 6, 7, 8],
                'init': ['k-means++', 'random']
            },
            'DBSCAN': {
                'eps': [0.5, 1.0, 1.5, 2.0],
                'min_samples': [3, 5, 10, 15]
            }
        }
        
        print(f"‚úÖ {len(self.clustering_models)} mod√®les de clustering configur√©s")
    
    def train_classification_models(self, X_train, X_test, y_train, y_test, optimize=True):
        """Entra√Æner les mod√®les de classification"""
        print("\nüöÄ Entra√Ænement des mod√®les de classification...")
        
        results = {}
        
        for name, model in self.classification_models.items():
            print(f"\nüìà Entra√Ænement {name}...")
            
            try:
                if optimize and name in self.param_grids:
                    # Optimisation avec GridSearch
                    print(f"   üîç Optimisation des hyperparam√®tres...")
                    grid_search = GridSearchCV(
                        model, self.param_grids[name], 
                        cv=3, scoring='f1_weighted', n_jobs=-1, verbose=0
                    )
                    grid_search.fit(X_train, y_train)
                    best_model = grid_search.best_estimator_
                    print(f"   ‚úÖ Meilleurs param√®tres: {grid_search.best_params_}")
                else:
                    # Entra√Ænement standard
                    best_model = model
                    best_model.fit(X_train, y_train)
                
                # Pr√©dictions
                y_pred = best_model.predict(X_test)
                y_pred_proba = best_model.predict_proba(X_test)[:, 1] if hasattr(best_model, 'predict_proba') else None
                
                # M√©triques
                metrics = {
                    'accuracy': accuracy_score(y_test, y_pred),
                    'precision': precision_score(y_test, y_pred, average='weighted'),
                    'recall': recall_score(y_test, y_pred, average='weighted'),
                    'f1': f1_score(y_test, y_pred, average='weighted')
                }
                
                if y_pred_proba is not None and len(np.unique(y_test)) == 2:
                    metrics['roc_auc'] = roc_auc_score(y_test, y_pred_proba)
                
                # Cross-validation
                cv_scores = cross_val_score(best_model, X_train, y_train, cv=5, scoring='f1_weighted')
                metrics['cv_mean'] = cv_scores.mean()
                metrics['cv_std'] = cv_scores.std()
                
                results[name] = {
                    'model': best_model,
                    'metrics': metrics,
                    'predictions': y_pred,
                    'probabilities': y_pred_proba
                }
                
                print(f"   ‚úÖ {name} - F1: {metrics['f1']:.3f}, Accuracy: {metrics['accuracy']:.3f}")
                
            except Exception as e:
                print(f"   ‚ùå Erreur avec {name}: {e}")
                continue
        
        self.classification_results = results
        return results
    
    def train_clustering_models(self, X, optimize=True):
        """Entra√Æner les mod√®les de clustering"""
        print("\nüéØ Entra√Ænement des mod√®les de clustering...")
        
        results = {}
        
        for name, model in self.clustering_models.items():
            print(f"\nüìä Clustering avec {name}...")
            
            try:
                best_score = -1
                best_model = None
                best_params = None
                
                if optimize and name in self.clustering_params:
                    # Test diff√©rents param√®tres
                    for param_combo in self._generate_param_combinations(self.clustering_params[name]):
                        temp_model = self.clustering_models[name].__class__(**param_combo, random_state=42)
                        
                        try:
                            labels = temp_model.fit_predict(X)
                            
                            if len(np.unique(labels)) > 1:  # Au moins 2 clusters
                                score = silhouette_score(X, labels)
                                
                                if score > best_score:
                                    best_score = score
                                    best_model = temp_model
                                    best_params = param_combo
                        except:
                            continue
                else:
                    # Param√®tres par d√©faut
                    best_model = model
                    labels = best_model.fit_predict(X)
                    if len(np.unique(labels)) > 1:
                        best_score = silhouette_score(X, labels)
                
                if best_model is not None:
                    final_labels = best_model.fit_predict(X)
                    
                    results[name] = {
                        'model': best_model,
                        'labels': final_labels,
                        'silhouette_score': best_score,
                        'n_clusters': len(np.unique(final_labels)),
                        'best_params': best_params
                    }
                    
                    print(f"   ‚úÖ {name} - Silhouette: {best_score:.3f}, Clusters: {len(np.unique(final_labels))}")
                
            except Exception as e:
                print(f"   ‚ùå Erreur avec {name}: {e}")
                continue
        
        self.clustering_results = results
        return results
    
    def _generate_param_combinations(self, param_grid):
        """G√©n√©rer les combinaisons de param√®tres"""
        from itertools import product
        
        keys = param_grid.keys()
        values = param_grid.values()
        
        for combination in product(*values):
            yield dict(zip(keys, combination))
    
    def evaluate_models(self, X_test, y_test):
        """√âvaluer et comparer les mod√®les"""
        print("\nüìä √âvaluation des mod√®les...")
        
        if hasattr(self, 'classification_results'):
            # Comparaison des mod√®les de classification
            comparison_data = []
            
            for name, result in self.classification_results.items():
                metrics = result['metrics']
                comparison_data.append({
                    'Model': name,
                    'Accuracy': metrics['accuracy'],
                    'Precision': metrics['precision'],
                    'Recall': metrics['recall'],
                    'F1-Score': metrics['f1'],
                    'ROC-AUC': metrics.get('roc_auc', 'N/A'),
                    'CV Mean': metrics['cv_mean'],
                    'CV Std': metrics['cv_std']
                })
            
            comparison_df = pd.DataFrame(comparison_data)
            print("\nüèÜ Comparaison des mod√®les de classification:")
            print(comparison_df.round(3))
            
            # S√©lectionner le meilleur mod√®le
            best_model_name = comparison_df.loc[comparison_df['F1-Score'].idxmax(), 'Model']
            self.best_model = self.classification_results[best_model_name]['model']
            self.best_model_name = best_model_name
            
            print(f"\nü•á Meilleur mod√®le: {best_model_name}")
            
            return comparison_df
    
    def generate_detailed_report(self, X_test, y_test, output_dir="reports"):
        """G√©n√©rer un rapport d√©taill√©"""
        print("\nüìã G√©n√©ration du rapport d√©taill√©...")
        
        os.makedirs(output_dir, exist_ok=True)
        
        if hasattr(self, 'best_model') and self.best_model is not None:
            # Rapport de classification
            y_pred = self.best_model.predict(X_test)
            
            report = classification_report(y_test, y_pred, output_dict=True)
            report_df = pd.DataFrame(report).transpose()
            
            # Sauvegarder le rapport
            report_df.to_csv(f"{output_dir}/classification_report.csv")
            
            # Matrice de confusion
            cm = confusion_matrix(y_test, y_pred)
            
            plt.figure(figsize=(8, 6))
            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
            plt.title(f'Matrice de Confusion - {self.best_model_name}')
            plt.ylabel('Vrai Label')
            plt.xlabel('Pr√©diction')
            plt.savefig(f"{output_dir}/confusion_matrix.png", dpi=300, bbox_inches='tight')
            plt.close()
            
            # Feature importance si disponible
            if hasattr(self.best_model, 'feature_importances_'):
                feature_names = [f'Feature_{i}' for i in range(len(self.best_model.feature_importances_))]
                importance_df = pd.DataFrame({
                    'feature': feature_names,
                    'importance': self.best_model.feature_importances_
                }).sort_values('importance', ascending=False)
                
                plt.figure(figsize=(12, 8))
                sns.barplot(data=importance_df.head(20), x='importance', y='feature')
                plt.title('Top 20 Feature Importances')
                plt.tight_layout()
                plt.savefig(f"{output_dir}/feature_importance.png", dpi=300, bbox_inches='tight')
                plt.close()
                
                importance_df.to_csv(f"{output_dir}/feature_importance.csv", index=False)
            
            print(f"‚úÖ Rapport sauvegard√© dans {output_dir}/")
    
    def save_models(self, output_dir="models"):
        """Sauvegarder les mod√®les entra√Æn√©s"""
        print("\nüíæ Sauvegarde des mod√®les...")
        
        os.makedirs(output_dir, exist_ok=True)
        
        if hasattr(self, 'best_model') and self.best_model is not None:
            model_path = f"{output_dir}/best_model_{self.best_model_name}.joblib"
            joblib.dump(self.best_model, model_path)
            print(f"‚úÖ Meilleur mod√®le sauvegard√©: {model_path}")
        
        # Sauvegarder tous les mod√®les de classification
        if hasattr(self, 'classification_results'):
            for name, result in self.classification_results.items():
                model_path = f"{output_dir}/model_{name}.joblib"
                joblib.dump(result['model'], model_path)
            print(f"‚úÖ {len(self.classification_results)} mod√®les de classification sauvegard√©s")
        
        # Sauvegarder les mod√®les de clustering
        if hasattr(self, 'clustering_results'):
            for name, result in self.clustering_results.items():
                model_path = f"{output_dir}/clustering_{name}.joblib"
                joblib.dump(result['model'], model_path)
            print(f"‚úÖ {len(self.clustering_results)} mod√®les de clustering sauvegard√©s")
    
    def predict_new_data(self, X_new):
        """Faire des pr√©dictions sur de nouvelles donn√©es"""
        if hasattr(self, 'best_model') and self.best_model is not None:
            predictions = self.best_model.predict(X_new)
            probabilities = None
            
            if hasattr(self.best_model, 'predict_proba'):
                probabilities = self.best_model.predict_proba(X_new)
            
            return predictions, probabilities
        else:
            print("‚ùå Aucun mod√®le entra√Æn√© disponible")
            return None, None

def main():
    """Fonction principale pour tester l'entra√Ænement"""
    trainer = InsuranceModelTrainer()
    
    # Charger les donn√©es (√† adapter selon votre preprocessing)
    data_path = "data/processed/processed_data.csv"
    df = trainer.load_processed_data(data_path)
    
    if df is not None:
        # Exemple d'utilisation avec des donn√©es factices
        print("‚ö†Ô∏è Utilisation de donn√©es factices pour la d√©monstration")
        
        # Cr√©er des donn√©es d'exemple
        np.random.seed(42)
        X = np.random.randn(1000, 10)
        y = np.random.choice([0, 1], 1000)
        
        # Split
        from sklearn.model_selection import train_test_split
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        
        # Classification
        trainer.setup_classification_models()
        classification_results = trainer.train_classification_models(X_train, X_test, y_train, y_test)
        
        # Clustering
        trainer.setup_clustering_models()
        clustering_results = trainer.train_clustering_models(X)
        
        # √âvaluation
        comparison_df = trainer.evaluate_models(X_test, y_test)
        
        # Rapport et sauvegarde
        trainer.generate_detailed_report(X_test, y_test)
        trainer.save_models()
        
        return trainer

if __name__ == "__main__":
    main()